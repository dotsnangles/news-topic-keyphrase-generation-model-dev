# 신문기사 핵심어 추출 모델 개발

## 1. 모델 입출력 개요

- 신문기사의 제목과 본문 내용을 함께 입력으로 받아 핵심어 10개를 출력
- 모델의 생성 방식에 따라 입력은 prefix와 postfix를 포함

### 입력 예시
```
generate keyphrases: SK하이닉스 1천만주 공매도 '폭탄'...오늘 공매도 거래 금지 외국인 대량 거래..."교환사채 헤지 차원에서 공매도한 듯"SK하이닉스[촬영 안 철 수, 재판매 및 DB금지](서울=연합뉴스) 채새롬 기자 = SK하이닉스가 전날 1천만주 넘는 공매도 '폭탄'을 맞아 공매도 과열종목으로 지정됐다.한국거래소는 5일 SK하이닉스에 대해 공매도 과열종목으로 지정하고, 정규시장 및 시간외시장에서 SK하이닉스의 공매도 거래를 금지한다고 공시했다.공매도 금지일인 이날 주가가 5% 이상 하락하면 공매도 금지 기간이 연장된다.거래소에 따르면 SK하이닉스 공매도 물량은 지난 4일 하루에만 1천만6천643주였다. 거래대금은 8천362억원으로, 3일 공매도 거래대금(96억원)의 87배에 달한다.SK하이닉스의 전날 전체 거래량 중 공매도 거래량의 비중은 59.53%이다.SK하이닉스에 공매도가 쏟아지면서 코스피 공매도 거래대금도 3일 6천285억원에서 4일 1조3천998억원으로 급증했다.다만 업계에서는 SK하이닉스 공매도가 불공정거래와 연관돼 있을 가능성은 낮다고 보고 있다.전날 공매도 물량의 대부분은 장 개시 전, 장 개시 후 블록딜(시간 외 대량매매) 방식의 외국인 공매도 물량이다.SK하이닉스는 4일 2조2천억원대의 해외 교환사채(EB)를 발행했는데, 외국인 투자자들이 포트폴리오 균형을 맞추기 위해 공매도에 나섰을 가능성이 높다.교환사채는 투자자가 보유한 채권을 일정 기간 후 발행사가 보유한 회사 주식으로 교환할 수 있는 권리가 있는 사채다.익명을 요구한 한 증권사 애널리스트는 "외국인 투자자들이 해외 교환사채를 사고, 헤지(위험 회피) 차원에서 국내 증시에서 대량 공매도를 한 것으로 보인다"며 "실질적인 주식 공매도는 아닌 것으로 추정한다"고 말했다.SK하이닉스 주가는 전날 교환사채 발행 영향으로 3.10% 하락했지만, 이날 오전에는 0.5% 안팎의 강보합세를 보이고 있다.srchae@yna.co.kr
```
### 출력 예시
```
SK하이닉스; 공매도; 거래 금지; 외국인 대량 거래; 교환사채; 헤지; 공매도 물량; 불공정거래; 블록딜; 국내 증시
```

## 2. 베이스 모델

### paust/pko-t5-base
- 2억 2천만 매개변수
- Encoder-Decoder (Seq2Seq / conditional generation) 구조의 모델로 Encoder가 생성한 입력의 Context Vector를 기반으로 출력을 생성
- 모든 NLP 태스크를 Text-to-Text Transfer 방식으로 해결하고자 기획된 모델로 NLG뿐 아니라 NLU 태스크에도 광범위하게 활용되는 모델
- T5 1.1을 기반으로 한국어 공공 데이터를 활용해 PAUST에서 사전 학습

### EleutherAI/polyglot-ko-1.3b
- 13억 매개변수
- Decoder-only (CLM / open-ended generation) 구조의 모델로 NLG에 특화되어 있으나 높은 모델 복잡도를 통해 다양한 NLP 태스크 수행이 가능
- EleutherAI, Stability AI, TUNiB이 합작하여 개발한 Foundational SLM으로 한국어로 사전 학습된 Decoder-only 계열에서는 현재 가장 높은 완성도를 가진 모델

## 3. 훈련 데이터
- 신문기사 수집 DB에서 최신 신문기사 30만건 추출
- pko-t5-base 토크나이저로 분절 후 500~1000 사이의 길이의 샘플 100886건에 대한 라벨링을 수행 (GPT Turbo 3.5)
- 훈련 데이터의 품질을 유지 하기 위해 토큰화 후 라벨의 길이가 64 이하로 생성된 샘플 79472건을 훈련에 활용

## 4. 평가지표
현재 모델이 수행하는 키프레이즈 제너레이션 태스크는 요약에 준하는 경우로 라벨을 Ground Truth로 설정할 수는 없으나 GPT Turbo 3.5가 생성한 키프레이즈와의 유사성을 검토하기 위해 아래 두 평가 지표를 검증 과정에서 활용
- ROUGE
- F1@10

## 5. 훈련 성과
- 추론 결과 검토 결과 GPT Turbo 3.5와 비교했을 때 생성된 키프레이즈의 차이가 존재하나 두 모델 모두 신문기사에서 핵심에 해당하는 키프레이즈를 잘 탐지하는 모습을 보이고 있음
- Seq2Seq 계열인 paust/pko-t5-base가 더 작은 규모의 모델임에도 불구하고 상당히 깔끔한 결과물을 출력하는 것을 확인
  - 추론 시 사용 자원이 EleutherAI/polyglot-ko-1.3b에 비해 훨씬 적으며 현재 성능 차이가 미미하기 때문에 주력 모델로 현재 계획 중
- EleutherAI/polyglot-ko-1.3b의 경우 현재 추가 실험을 통해 아직 개선될 여지가 존재
  - eos 토큰과 pad 토큰을 동일하게 설정하는 일반적인 방법으로 훈련을 진행
  - F1@10가 더 높은 만큼 생성 기법을 조금 시험해볼 여지가 있음
  - 현재 가용 자원으로 사용 가능한 유일한 한국어 SLM인 만큼 계속해서 연구해볼 예정

### paust/pko-t5-base
[추론 결과](results/paust_pko_t5_base_v4_run_1.txt)
- 'rouge1': 67.8227,
- 'rouge2': 47.6712,
- 'rougeL': 55.9405,
- 'rougeLsum': 55.9405,
- 'F1@10': 60.2782,
- 'jaccard_similarity': 27.6543,
- 'gen_len': 48.3867

### EleutherAI/polyglot-ko-1.3b
[추론 결과](results/eleutherai_polyglot_ko_1.3b_v4_run_14.txt)
- 'rouge1': 49.7939,
- 'rouge2': 36.1631,
- 'rougeL': 41.84,
- 'rougeLsum': 41.84,
- 'F1@10': 62.6319,
- 'jaccard_similarity': 27.7235,
- 'gen_len': 64.0