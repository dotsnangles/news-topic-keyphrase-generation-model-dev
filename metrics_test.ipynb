{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, \n",
    "    T5TokenizerFast, T5ForConditionalGeneration, \n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, \n",
    "    AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from datasets import load_metric, Dataset\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import wandb\n",
    "import nltk\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGPU = torch.cuda.device_count()\n",
    "NCPU = os.cpu_count()\n",
    "NGPU, NCPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths and names\n",
    "\n",
    "DATA_PATH = 'data/model_dev/model_dev_v3.pickle'\n",
    "MODEL_CHECKPOINT = '.log/paust_pko_t5_base_v3_run_5/checkpoint-11310'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"generate keyphrases: \"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"input_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(examples[\"target_text\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_df).shuffle(seed=100).train_test_split(0.2, seed=100)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f279e416564d099b254cced6501e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/9346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc49b741279418e9bd7206c9a85ebb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/2337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9346\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2337\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_function, \n",
    "                                  batched=True, \n",
    "                                  num_proc=NCPU, \n",
    "                                  remove_columns=train_dataset.column_names)\n",
    "\n",
    "eval_dataset = eval_dataset.map(preprocess_function, \n",
    "                                batched=True, \n",
    "                                num_proc=NCPU, \n",
    "                                remove_columns=eval_dataset.column_names)\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = eval_dataset[:100]\n",
    "input_ids = torch.tensor(inputs['input_ids'])\n",
    "attention_mask = torch.tensor(inputs['attention_mask'])\n",
    "labels = tokenizer.batch_decode(inputs['labels'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "# print(f'inputs_decoded: {inputs_decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)\n",
    "predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [[label] for label in labels]\n",
    "# predictions = [[prediction] for prediction in predictions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_for_sampale(label, prediction):\n",
    "    return rouge.compute(references=[label], predictions=[prediction], tokenizer=komoran.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_for_batch(labels, predictions):\n",
    "    rouge_scores = None\n",
    "    \n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if rouge_scores == None:\n",
    "            rouge_scores = rouge_for_sampale(label, prediction)\n",
    "        else:\n",
    "            rouge_score = rouge_for_sampale(label, prediction)\n",
    "            for key in rouge_scores.keys():\n",
    "                rouge_scores[key] = rouge_scores[key] + rouge_score[key]\n",
    "    \n",
    "    for key in rouge_scores.keys():\n",
    "        rouge_scores[key] = rouge_scores[key] / len(labels)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6478344446493621,\n",
       " 'rouge2': 0.4362975104732406,\n",
       " 'rougeL': 0.5335291248203091,\n",
       " 'rougeLsum': 0.5335291248203091}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_for_batch(labels, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k_for_sample(label, prediction, k):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    label_set = set(label_lst)\n",
    "    # print(f'label_set: {label_set}')\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    prediction_set = set(prediction_lst)\n",
    "    # prediction_set = set(p[0] for p in predictions[:k])\n",
    "    # print(f'prediction_set: {prediction_set}')\n",
    "    \n",
    "    # calculate true positives, false positives, and false negatives\n",
    "    for keyphrase in prediction_set:\n",
    "        if keyphrase in label_set:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    for keyphrase in label_set:\n",
    "        if keyphrase not in prediction_set:\n",
    "            false_negatives += 1\n",
    "    \n",
    "    # print(f'true_positives: {true_positives}')    \n",
    "    # print(f'false_positives: {false_positives}')\n",
    "    # print(f'false_negatives: {false_negatives}')\n",
    "\n",
    "    # calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k_for_sample(label, prediction, k):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    label_set = set(label_lst)\n",
    "    # print(f'label_set: {label_set}')\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    prediction_set = set(prediction_lst)\n",
    "    # prediction_set = set(p[0] for p in predictions[:k])\n",
    "    # print(f'prediction_set: {prediction_set}')\n",
    "    \n",
    "    # calculate true positives, false positives, and false negatives\n",
    "    for keyphrase in prediction_set:\n",
    "        if keyphrase in label_set:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    for keyphrase in label_set:\n",
    "        if keyphrase not in prediction_set:\n",
    "            false_negatives += 1\n",
    "    \n",
    "    # print(f'true_positives: {true_positives}')    \n",
    "    # print(f'false_positives: {false_positives}')\n",
    "    # print(f'false_negatives: {false_negatives}')\n",
    "\n",
    "    # calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k_for_batch(labels, predictions, k):\n",
    "    f1_scores =[]\n",
    "\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        f1_scores.append(f1_score_at_k_for_sample(label, prediction, k))\n",
    "\n",
    "    # print(f1_scores)\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.20000000000000004, 0, 0.6, 0.4210526315789474, 0.3, 0.20000000000000004, 0.33333333333333326, 0.20000000000000004, 0.3, 0.5, 0.4000000000000001, 0.4000000000000001, 0.20000000000000004, 0.20000000000000004, 0.2105263157894737, 0.4000000000000001, 0.20000000000000004, 0.5555555555555556, 0.28571428571428564, 0.3157894736842105, 0.5, 0.6, 0.20000000000000004, 0.1111111111111111, 0.3157894736842105, 0.4444444444444445, 0.6, 0.4000000000000001, 0.3, 0.20000000000000004, 0.4000000000000001, 0.6, 0.5, 0.4000000000000001, 0.8000000000000002, 0.23529411764705882, 0.2105263157894737, 0.3, 0.4210526315789474, 0.4210526315789474, 0.6, 0.5263157894736842, 0.3, 0.5, 0.4000000000000001, 0.6, 0.4000000000000001, 0.2105263157894737, 0.10000000000000002, 0.3, 0.3, 0.3, 0.4000000000000001, 0.4000000000000001, 0.3, 0.5263157894736842, 0.7, 0.631578947368421, 0.5263157894736842, 0.6, 0.4000000000000001, 0.4000000000000001, 0.7368421052631577, 0.3, 0.3157894736842105, 0.4000000000000001, 0.631578947368421, 0.7, 0.4210526315789474, 0.3, 0.10526315789473685, 0.4000000000000001, 0.8421052631578948, 0.20000000000000004, 0.4000000000000001, 0.4000000000000001, 0.4000000000000001, 0.37499999999999994, 0.4000000000000001, 0.5, 0.4444444444444445, 0.3, 0.5, 0.10526315789473685, 0, 0.20000000000000004, 0.3, 0.631578947368421, 0.20000000000000004, 0.631578947368421, 0.4210526315789474, 0.10000000000000002, 0.4000000000000001, 0.3157894736842105, 0.20000000000000004, 0.4000000000000001, 0.4210526315789474, 0.4000000000000001, 0.10000000000000002]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38000686765934427"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_at_k_for_batch(labels, predictions, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score_at_k_for_sample(labels[9], prediction[9], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_for_sample(label, prediction, k):\n",
    "\n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    # print(label_lst)\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    # print(prediction_lst)\n",
    "\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(label_lst).intersection(prediction_lst)))\n",
    "    union = (len(label_lst) + len(prediction_lst)) - intersection\n",
    "\n",
    "    # print(union)\n",
    "    # print(intersection)\n",
    "\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_for_batch(labels, predictions, k):\n",
    "    jaccard_similarities =[]\n",
    "\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        jaccard_similarities.append(jaccard_similarity_for_sample(label, prediction, k))\n",
    "\n",
    "    print(jaccard_similarities)\n",
    "    return sum(jaccard_similarities) / len(jaccard_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.1111111111111111, 0.0, 0.42857142857142855, 0.25, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.25, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.25, 0.1111111111111111, 0.3333333333333333, 0.16666666666666666, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.1111111111111111, 0.05555555555555555, 0.17647058823529413, 0.25, 0.42857142857142855, 0.25, 0.17647058823529413, 0.1111111111111111, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.6666666666666666, 0.125, 0.1111111111111111, 0.17647058823529413, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.42857142857142855, 0.25, 0.1111111111111111, 0.05263157894736842, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.5384615384615384, 0.42857142857142855, 0.3333333333333333, 0.42857142857142855, 0.25, 0.25, 0.5384615384615384, 0.17647058823529413, 0.17647058823529413, 0.25, 0.42857142857142855, 0.5384615384615384, 0.25, 0.17647058823529413, 0.05263157894736842, 0.25, 0.6666666666666666, 0.1111111111111111, 0.25, 0.25, 0.25, 0.17647058823529413, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3333333333333333, 0.05555555555555555, 0.0, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.42857142857142855, 0.25, 0.05263157894736842, 0.25, 0.17647058823529413, 0.1111111111111111, 0.25, 0.25, 0.25, 0.05263157894736842]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2421258226637483"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity_for_batch(labels, predictions, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
