{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, \n",
    "    T5TokenizerFast, T5ForConditionalGeneration, \n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, \n",
    "    AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGPU = torch.cuda.device_count()\n",
    "NCPU = os.cpu_count()\n",
    "NGPU, NCPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths and names\n",
    "\n",
    "DATA_PATH = 'data/model_dev/model_dev_v3.pickle'\n",
    "MODEL_CHECKPOINT = '.log/paust_pko_t5_base_v3_run_5/checkpoint-11310'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"generate keyphrases: \"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"input_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(examples[\"target_text\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_df).shuffle(seed=100).train_test_split(0.2, seed=100)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72595d66b204b2e95b6b2d62733c9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/9346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a2afb53bae487aa39bc98b0a6d202a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/2337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9346\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2337\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_function, \n",
    "                                  batched=True, \n",
    "                                  num_proc=NCPU, \n",
    "                                  remove_columns=train_dataset.column_names)\n",
    "\n",
    "eval_dataset = eval_dataset.map(preprocess_function, \n",
    "                                batched=True, \n",
    "                                num_proc=NCPU, \n",
    "                                remove_columns=eval_dataset.column_names)\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = eval_dataset[:100]\n",
    "inputs = eval_dataset\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer.batch_decode(inputs['labels'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [01:59<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "predictions = None\n",
    "with torch.no_grad():\n",
    "    start = 0\n",
    "    for idx in tqdm(range(batch_size, len(input_ids), batch_size)):\n",
    "        ids, mask = input_ids[start:idx], attention_mask[start:idx]\n",
    "        ids, mask = torch.tensor(ids).to(device), torch.tensor(mask).to(device)\n",
    "        # print(start, idx)\n",
    "        prediction = model.generate(input_ids=ids, attention_mask=mask, max_length=64)\n",
    "        if predictions == None:\n",
    "            predictions = prediction.detach().cpu().tolist()\n",
    "        else:\n",
    "            predictions.extend(prediction.detach().cpu().tolist())\n",
    "        start = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_for_sampale(label, prediction):\n",
    "    return rouge.compute(references=[label], predictions=[prediction], tokenizer=komoran.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_for_batch(labels, predictions):\n",
    "    rouge_scores = None\n",
    "    \n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if rouge_scores == None:\n",
    "            rouge_scores = rouge_for_sampale(label, prediction)\n",
    "        else:\n",
    "            rouge_score = rouge_for_sampale(label, prediction)\n",
    "            for key in rouge_scores.keys():\n",
    "                rouge_scores[key] = rouge_scores[key] + rouge_score[key]\n",
    "    \n",
    "    for key in rouge_scores.keys():\n",
    "        rouge_scores[key] = rouge_scores[key] / len(labels)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6481759823542651,\n",
       " 'rouge2': 0.4481389571495031,\n",
       " 'rougeL': 0.5329436625976698,\n",
       " 'rougeLsum': 0.5329436625976698}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_for_batch(labels, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### V1\n",
    "\n",
    "# def f1_score_at_k_for_sample(label_str, prediction_str, k):\n",
    "#     true_positives = 0\n",
    "#     false_positives = 0\n",
    "#     false_negatives = 0\n",
    "    \n",
    "#     # convert label and prediction strings to sets of key-phrases\n",
    "#     label_lst = [key_phrase.strip() for key_phrase in label_str.split(';') if key_phrase != '']\n",
    "#     label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "#     label_set = set(label_lst)\n",
    "    \n",
    "#     # split the predicted key-phrases and their scores\n",
    "#     prediction_lst = [key_phrase.strip() for key_phrase in prediction_str.split(';') if key_phrase != '']\n",
    "#     prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "#     prediction_set = set(prediction_lst)\n",
    "    \n",
    "#     # calculate true positives, false positives, and false negatives\n",
    "#     for keyphrase in prediction_set:\n",
    "#         if keyphrase in label_set:\n",
    "#             true_positives += 1\n",
    "#         else:\n",
    "#             false_positives += 1\n",
    "    \n",
    "#     for keyphrase in label_set:\n",
    "#         if keyphrase not in prediction_set:\n",
    "#             false_negatives += 1\n",
    "    \n",
    "#     # calculate precision, recall, and F1 score\n",
    "#     precision = true_positives / (true_positives + false_positives)\n",
    "#     recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "#     if precision == 0 or recall == 0:\n",
    "#         return 0\n",
    "    \n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "#     return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### V2\n",
    "\n",
    "def f1_score_at_k_for_sample(label_str, prediction_str, k):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label_str.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction_str.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    \n",
    "    # calculate true positives, false positives, and false negatives\n",
    "    for keyphrase in prediction_lst:\n",
    "        similarity = False\n",
    "        for label in label_lst:\n",
    "            if keyphrase in label or label in keyphrase:\n",
    "                similarity = True\n",
    "                break\n",
    "        if similarity == True:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    for label in label_lst:\n",
    "        similarity = False\n",
    "        for keyphrase in prediction_lst:\n",
    "            if label in keyphrase or keyphrase in label:\n",
    "                similarity = True\n",
    "                break\n",
    "        if similarity == False:\n",
    "            false_negatives += 1            \n",
    "\n",
    "    # calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k_for_batch(labels, predictions, k):\n",
    "    f1_scores =[]\n",
    "\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        f1_scores.append(f1_score_at_k_for_sample(label, prediction, k))\n",
    "\n",
    "    # print(f1_scores)\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597830229966394"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_at_k_for_batch(labels, predictions, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score_at_k_for_sample(labels[9], prediction[9], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_for_sample(label, prediction, k):\n",
    "\n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    # print(label_lst)\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    # print(prediction_lst)\n",
    "\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(label_lst).intersection(prediction_lst)))\n",
    "    union = (len(label_lst) + len(prediction_lst)) - intersection\n",
    "\n",
    "    # print(union)\n",
    "    # print(intersection)\n",
    "\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_for_batch(labels, predictions, k):\n",
    "    jaccard_similarities =[]\n",
    "\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        jaccard_similarities.append(jaccard_similarity_for_sample(label, prediction, k))\n",
    "\n",
    "    print(jaccard_similarities)\n",
    "    return sum(jaccard_similarities) / len(jaccard_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.1111111111111111, 0.0, 0.42857142857142855, 0.25, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.25, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.25, 0.1111111111111111, 0.3333333333333333, 0.16666666666666666, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.1111111111111111, 0.05555555555555555, 0.17647058823529413, 0.25, 0.42857142857142855, 0.25, 0.17647058823529413, 0.1111111111111111, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.6666666666666666, 0.125, 0.1111111111111111, 0.17647058823529413, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.42857142857142855, 0.25, 0.1111111111111111, 0.05263157894736842, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.5384615384615384, 0.42857142857142855, 0.3333333333333333, 0.42857142857142855, 0.25, 0.25, 0.5384615384615384, 0.17647058823529413, 0.17647058823529413, 0.25, 0.42857142857142855, 0.5384615384615384, 0.25, 0.17647058823529413, 0.05263157894736842, 0.25, 0.6666666666666666, 0.1111111111111111, 0.25, 0.25, 0.25, 0.17647058823529413, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3333333333333333, 0.05555555555555555, 0.0, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.42857142857142855, 0.25, 0.05263157894736842, 0.25, 0.17647058823529413, 0.1111111111111111, 0.25, 0.25, 0.25, 0.05263157894736842, 0.17647058823529413, 0.42857142857142855, 0.05555555555555555, 0.6666666666666666, 0.23529411764705882, 0.3125, 0.3333333333333333, 0.3333333333333333, 0.5384615384615384, 0.05263157894736842, 0.25, 0.3333333333333333, 0.25, 0.3333333333333333, 0.5384615384615384, 0.25, 0.05263157894736842, 0.1111111111111111, 0.17647058823529413, 0.25, 0.3333333333333333, 0.42857142857142855, 0.5384615384615384, 0.42857142857142855, 0.3333333333333333, 0.058823529411764705, 0.1111111111111111, 0.3333333333333333, 0.25, 0.25, 0.35714285714285715, 0.6666666666666666, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.17647058823529413, 0.25, 0.17647058823529413, 0.17647058823529413, 0.25, 0.29411764705882354, 0.3333333333333333, 0.1111111111111111, 0.0, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.5384615384615384, 0.25, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.25, 0.17647058823529413, 0.25, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.25, 0.1111111111111111, 0.0, 0.25, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.25, 0.3333333333333333, 0.17647058823529413, 0.1111111111111111, 0.25, 0.42857142857142855, 0.25, 0.25, 0.1111111111111111, 0.23529411764705882, 0.25, 0.5384615384615384, 0.17647058823529413, 0.25, 0.23529411764705882, 0.17647058823529413, 0.3333333333333333, 0.25, 0.05263157894736842, 0.17647058823529413, 0.05263157894736842, 0.17647058823529413, 0.3333333333333333, 0.25, 0.05263157894736842, 0.1111111111111111, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.1111111111111111, 0.11764705882352941, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.5384615384615384, 0.42857142857142855, 0.1111111111111111, 0.35714285714285715, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.25, 0.4, 0.3333333333333333, 0.42857142857142855, 0.36363636363636365, 0.25, 0.25, 0.17647058823529413, 0.1875, 0.42857142857142855, 0.17647058823529413, 0.25, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.5384615384615384, 0.05263157894736842, 0.42857142857142855, 0.17647058823529413, 0.25, 0.42857142857142855, 0.42857142857142855, 0.05, 0.3333333333333333, 0.5384615384615384, 0.42857142857142855, 0.3333333333333333, 0.6666666666666666, 0.1111111111111111, 0.25, 0.3333333333333333, 0.25, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.5384615384615384, 0.11764705882352941, 0.0, 0.17647058823529413, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.23529411764705882, 0.25, 0.25, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.25, 0.25, 0.25, 0.3125, 0.42857142857142855, 0.35714285714285715, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.05263157894736842, 0.6666666666666666, 0.3333333333333333, 0.05263157894736842, 0.25, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.25, 0.17647058823529413, 0.05263157894736842, 0.3333333333333333, 0.17647058823529413, 0.25, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.3125, 0.23529411764705882, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.42857142857142855, 0.6666666666666666, 0.42857142857142855, 0.1111111111111111, 0.1111111111111111, 0.25, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.1111111111111111, 0.42857142857142855, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.7272727272727273, 0.17647058823529413, 0.4, 0.3333333333333333, 0.25, 0.25, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.1111111111111111, 0.16666666666666666, 0.42857142857142855, 0.17647058823529413, 0.17647058823529413, 0.25, 0.25, 0.05263157894736842, 0.25, 0.42857142857142855, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.42857142857142855, 0.25, 0.17647058823529413, 0.05263157894736842, 0.05263157894736842, 0.3333333333333333, 0.5384615384615384, 0.3333333333333333, 0.1111111111111111, 0.5384615384615384, 0.42857142857142855, 0.25, 0.11764705882352941, 0.42857142857142855, 0.25, 0.17647058823529413, 0.25, 0.5384615384615384, 0.42857142857142855, 0.42857142857142855, 0.26666666666666666, 0.17647058823529413, 0.25, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.25, 0.25, 0.25, 0.25, 0.3333333333333333, 0.25, 0.25, 0.25, 0.17647058823529413, 0.6666666666666666, 0.25, 0.17647058823529413, 0.23529411764705882, 0.1111111111111111, 0.17647058823529413, 0.25, 0.17647058823529413, 0.25, 0.17647058823529413, 0.17647058823529413, 0.25, 0.3333333333333333, 0.42857142857142855, 0.17647058823529413, 0.5384615384615384, 0.25, 0.25, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.25, 0.23529411764705882, 0.25, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.25, 0.05555555555555555, 0.3333333333333333, 0.25, 0.3333333333333333, 0.17647058823529413, 0.25, 0.25, 0.42857142857142855, 0.11764705882352941, 0.05555555555555555, 0.3333333333333333, 0.05263157894736842, 0.25, 0.3333333333333333, 0.42857142857142855, 0.25, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3333333333333333, 0.25, 0.42857142857142855, 0.25, 0.5384615384615384, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.10526315789473684, 0.25, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.25, 0.05263157894736842, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.10526315789473684, 0.25, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.42857142857142855, 0.25, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.25, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.5384615384615384, 0.25, 0.05263157894736842, 0.17647058823529413, 0.1111111111111111, 0.3125, 0.17647058823529413, 0.25, 0.25, 0.05263157894736842, 0.42857142857142855, 0.16666666666666666, 0.25, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.25, 0.25, 0.25, 0.35714285714285715, 0.17647058823529413, 0.1111111111111111, 0.25, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.05263157894736842, 0.42857142857142855, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.0, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.11764705882352941, 0.05263157894736842, 0.17647058823529413, 0.17647058823529413, 0.25, 0.05263157894736842, 0.29411764705882354, 0.1111111111111111, 0.26666666666666666, 0.5384615384615384, 0.1111111111111111, 0.6666666666666666, 0.17647058823529413, 0.42857142857142855, 0.11764705882352941, 0.46153846153846156, 0.3333333333333333, 0.42857142857142855, 0.42857142857142855, 0.25, 0.25, 0.1875, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.2222222222222222, 0.5384615384615384, 0.42857142857142855, 0.3333333333333333, 0.25, 0.17647058823529413, 0.25, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.16666666666666666, 0.3333333333333333, 0.25, 0.5384615384615384, 0.3333333333333333, 0.25, 0.5384615384615384, 0.3333333333333333, 0.05263157894736842, 0.0, 0.3333333333333333, 0.05263157894736842, 0.25, 0.25, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.5384615384615384, 0.35714285714285715, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.25, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.1111111111111111, 0.25, 0.5384615384615384, 0.1111111111111111, 0.17647058823529413, 0.25, 0.25, 0.3333333333333333, 0.17647058823529413, 0.25, 0.6666666666666666, 0.3333333333333333, 0.1111111111111111, 0.35714285714285715, 0.25, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.1111111111111111, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.05263157894736842, 0.17647058823529413, 0.1111111111111111, 0.46153846153846156, 0.3333333333333333, 0.23529411764705882, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.25, 0.46153846153846156, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.25, 0.26666666666666666, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.25, 0.5384615384615384, 0.0, 0.3333333333333333, 0.46153846153846156, 0.25, 0.05263157894736842, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.25, 0.1111111111111111, 0.25, 0.5384615384615384, 0.25, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.25, 0.42857142857142855, 0.3333333333333333, 0.26666666666666666, 0.25, 0.17647058823529413, 0.25, 0.11764705882352941, 0.3333333333333333, 0.1111111111111111, 0.25, 0.17647058823529413, 0.1111111111111111, 0.05263157894736842, 0.25, 0.1111111111111111, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.23529411764705882, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.3333333333333333, 0.5384615384615384, 0.25, 0.26666666666666666, 0.25, 0.42857142857142855, 0.25, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3125, 0.25, 0.3333333333333333, 0.6666666666666666, 0.17647058823529413, 0.25, 0.17647058823529413, 0.17647058823529413, 0.125, 0.1111111111111111, 0.25, 0.42857142857142855, 0.3125, 0.05263157894736842, 0.25, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.25, 0.42857142857142855, 0.25, 0.3333333333333333, 0.25, 0.3333333333333333, 0.25, 0.5384615384615384, 0.1111111111111111, 0.42857142857142855, 0.25, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.25, 0.17647058823529413, 0.1875, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.25, 0.05263157894736842, 0.3333333333333333, 0.3333333333333333, 0.05263157894736842, 0.25, 0.5384615384615384, 0.17647058823529413, 0.5384615384615384, 0.42857142857142855, 0.42857142857142855, 0.4, 0.25, 0.42857142857142855, 0.25, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.17647058823529413, 0.3333333333333333, 0.6666666666666666, 0.25, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.25, 0.3333333333333333, 0.6666666666666666, 0.1111111111111111, 0.25, 0.25, 0.05263157894736842, 0.17647058823529413, 0.5384615384615384, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.25, 0.1111111111111111, 0.05, 0.42857142857142855, 0.25, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.25, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.07142857142857142, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.5384615384615384, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.25, 0.3333333333333333, 0.42857142857142855, 0.1111111111111111, 0.3333333333333333, 0.25, 0.4, 0.42857142857142855, 0.1111111111111111, 0.25, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.25, 0.25, 0.1111111111111111, 0.42857142857142855, 0.1111111111111111, 0.1111111111111111, 0.8181818181818182, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.4, 0.42857142857142855, 0.25, 0.25, 0.25, 0.17647058823529413, 0.25, 0.25, 0.26666666666666666, 0.42857142857142855, 0.42857142857142855, 0.25, 0.4, 0.25, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.05263157894736842, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.5384615384615384, 0.3333333333333333, 0.25, 0.1111111111111111, 0.3333333333333333, 0.0, 0.1111111111111111, 0.3333333333333333, 0.0, 0.26666666666666666, 0.1111111111111111, 0.05263157894736842, 0.3333333333333333, 0.42857142857142855, 0.25, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.5384615384615384, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.35714285714285715, 0.42857142857142855, 0.35714285714285715, 0.1111111111111111, 0.17647058823529413, 0.5384615384615384, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.25, 0.42857142857142855, 0.1111111111111111, 0.25, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.05263157894736842, 0.1111111111111111, 0.42857142857142855, 0.25, 0.17647058823529413, 0.25, 0.25, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.25, 0.3333333333333333, 0.42857142857142855, 0.25, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.3333333333333333, 0.0, 0.42857142857142855, 0.25, 0.5384615384615384, 0.25, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.42857142857142855, 0.1875, 0.3333333333333333, 0.17647058823529413, 0.25, 0.16666666666666666, 0.17647058823529413, 0.35714285714285715, 0.17647058823529413, 0.17647058823529413, 0.25, 0.05263157894736842, 0.5384615384615384, 0.25, 0.0, 0.25, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.25, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.25, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.25, 0.3333333333333333, 0.3333333333333333, 0.8181818181818182, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.25, 0.25, 0.3333333333333333, 0.25, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.05263157894736842, 0.17647058823529413, 0.5, 0.23529411764705882, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666, 0.3333333333333333, 0.42857142857142855, 0.42857142857142855, 0.5384615384615384, 0.11764705882352941, 0.25, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.05263157894736842, 0.42857142857142855, 0.25, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.05263157894736842, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.42857142857142855, 0.5384615384615384, 0.17647058823529413, 0.25, 0.3333333333333333, 0.16666666666666666, 0.17647058823529413, 0.42857142857142855, 0.05263157894736842, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.25, 0.25, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.1111111111111111, 0.3333333333333333, 0.26666666666666666, 0.25, 0.25, 0.25, 0.25, 0.0, 0.3125, 0.25, 0.25, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.23529411764705882, 0.25, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.25, 0.42857142857142855, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.4, 0.1875, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.1875, 0.05555555555555555, 0.42857142857142855, 0.46153846153846156, 0.17647058823529413, 0.25, 0.17647058823529413, 0.1111111111111111, 0.1111111111111111, 0.25, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.25, 0.3333333333333333, 0.05263157894736842, 0.1875, 0.3333333333333333, 0.17647058823529413, 0.1111111111111111, 0.25, 0.25, 0.17647058823529413, 0.05263157894736842, 0.17647058823529413, 0.3333333333333333, 0.25, 0.25, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.11764705882352941, 0.3333333333333333, 0.26666666666666666, 0.17647058823529413, 0.5384615384615384, 0.1111111111111111, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.25, 0.17647058823529413, 0.25, 0.26666666666666666, 0.25, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.3333333333333333, 0.17647058823529413, 0.6666666666666666, 0.17647058823529413, 0.1111111111111111, 0.23529411764705882, 0.16666666666666666, 0.2, 0.1111111111111111, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.1875, 0.42857142857142855, 0.1111111111111111, 0.25, 0.5384615384615384, 0.5384615384615384, 0.3125, 0.42857142857142855, 0.25, 0.25, 0.25, 0.17647058823529413, 0.5384615384615384, 0.25, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.05263157894736842, 0.25, 0.25, 0.17647058823529413, 0.17647058823529413, 0.25, 0.3333333333333333, 0.1111111111111111, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.3125, 0.3333333333333333, 0.25, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.25, 0.05263157894736842, 0.11764705882352941, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.23529411764705882, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.25, 0.17647058823529413, 0.1111111111111111, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.25, 0.3333333333333333, 0.25, 0.25, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.0, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.25, 0.25, 0.05263157894736842, 0.1111111111111111, 0.17647058823529413, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.4, 0.25, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.25, 0.17647058823529413, 0.05263157894736842, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.05263157894736842, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.5384615384615384, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.25, 0.17647058823529413, 0.05555555555555555, 0.17647058823529413, 0.25, 0.1111111111111111, 0.5384615384615384, 0.25, 0.17647058823529413, 0.42857142857142855, 0.25, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.42857142857142855, 0.25, 0.3333333333333333, 0.16666666666666666, 0.05263157894736842, 0.17647058823529413, 0.11764705882352941, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.05, 0.1111111111111111, 0.1111111111111111, 0.25, 0.42857142857142855, 0.1111111111111111, 0.05263157894736842, 0.1111111111111111, 0.42857142857142855, 0.42857142857142855, 0.25, 0.25, 0.17647058823529413, 0.1111111111111111, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.25, 0.17647058823529413, 0.25, 0.25, 0.1111111111111111, 0.17647058823529413, 0.25, 0.17647058823529413, 0.42857142857142855, 0.25, 0.42857142857142855, 0.5, 0.1111111111111111, 0.05263157894736842, 0.42857142857142855, 0.25, 0.1111111111111111, 0.25, 0.25, 0.3333333333333333, 0.25, 0.25, 0.25, 0.42857142857142855, 0.05263157894736842, 0.42857142857142855, 0.3333333333333333, 0.5384615384615384, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.05263157894736842, 0.05263157894736842, 0.17647058823529413, 0.5384615384615384, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.42857142857142855, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.42857142857142855, 0.1111111111111111, 0.42857142857142855, 0.17647058823529413, 0.25, 0.42857142857142855, 0.25, 0.16666666666666666, 0.25, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.05263157894736842, 0.25, 0.42857142857142855, 0.25, 0.3333333333333333, 0.16666666666666666, 0.2857142857142857, 0.3333333333333333, 0.25, 0.42857142857142855, 0.1111111111111111, 0.6666666666666666, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.10526315789473684, 0.25, 0.1111111111111111, 0.25, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.25, 0.25, 0.17647058823529413, 0.17647058823529413, 0.3125, 0.3333333333333333, 0.17647058823529413, 0.25, 0.05263157894736842, 0.17647058823529413, 0.25, 0.25, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.25, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.05263157894736842, 0.17647058823529413, 0.42857142857142855, 0.25, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.25, 0.6666666666666666, 0.35714285714285715, 0.3333333333333333, 0.25, 0.25, 0.23529411764705882, 0.3333333333333333, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.25, 0.3333333333333333, 0.42857142857142855, 0.3333333333333333, 0.23529411764705882, 0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.25, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.5384615384615384, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.5384615384615384, 0.25, 0.3333333333333333, 0.1111111111111111, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.05263157894736842, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.1111111111111111, 0.17647058823529413, 0.25, 0.3333333333333333, 0.23529411764705882, 0.25, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.25, 0.25, 0.25, 0.25, 0.25, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111, 0.1111111111111111, 0.5384615384615384, 0.17647058823529413, 0.3333333333333333, 0.6666666666666666, 0.42857142857142855, 0.3333333333333333, 0.25, 0.05263157894736842, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.1, 0.25, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.42857142857142855, 0.17647058823529413, 0.25, 0.17647058823529413, 0.5384615384615384, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.42857142857142855, 0.25, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.5384615384615384, 0.42857142857142855, 0.25, 0.25, 0.4, 0.16666666666666666, 0.25, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.23529411764705882, 0.17647058823529413, 0.26666666666666666, 0.3333333333333333, 0.25, 0.05263157894736842, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.1111111111111111, 0.42857142857142855, 0.17647058823529413, 0.25, 0.42857142857142855, 0.3333333333333333, 0.42857142857142855, 0.17647058823529413, 0.25, 0.25, 0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.17647058823529413, 0.5384615384615384, 0.1111111111111111, 0.5384615384615384, 0.05263157894736842, 0.25, 0.25, 0.3333333333333333, 0.4, 0.05263157894736842, 0.25, 0.25, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.25, 0.25, 0.25, 0.3333333333333333, 0.17647058823529413, 0.25, 0.3333333333333333, 0.1111111111111111, 0.25, 0.1111111111111111, 0.25, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.3333333333333333, 0.1111111111111111, 0.42857142857142855, 0.25, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.25, 0.17647058823529413, 0.0, 0.3333333333333333, 0.25, 0.1875, 0.25, 0.25, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.25, 0.25, 0.25, 0.05263157894736842, 0.3333333333333333, 0.6666666666666666, 0.3125, 0.3333333333333333, 0.25, 0.42857142857142855, 0.0, 0.3125, 0.17647058823529413, 0.17647058823529413, 0.1875, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.4, 0.5384615384615384, 0.1111111111111111, 0.25, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.42857142857142855, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.42857142857142855, 0.1111111111111111, 0.3125, 0.3333333333333333, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.25, 0.3333333333333333, 0.17647058823529413, 0.25, 0.17647058823529413, 0.17647058823529413, 0.25, 0.25, 0.25, 0.17647058823529413, 0.5714285714285714, 0.17647058823529413, 0.17647058823529413, 0.25, 0.17647058823529413, 0.05263157894736842, 0.5384615384615384, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.25, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.42857142857142855, 0.17647058823529413, 0.23529411764705882, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.25, 0.1111111111111111, 0.1111111111111111, 0.3333333333333333, 0.1875, 0.17647058823529413, 0.25, 0.6666666666666666, 0.25, 0.35714285714285715, 0.17647058823529413, 0.05263157894736842, 0.3333333333333333, 0.05263157894736842, 0.42857142857142855, 0.17647058823529413, 0.25, 0.1111111111111111, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.25, 0.5384615384615384, 0.17647058823529413, 0.17647058823529413, 0.05263157894736842, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.11764705882352941, 0.42857142857142855, 0.42857142857142855, 0.5384615384615384, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.05263157894736842, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.25, 0.1111111111111111, 0.17647058823529413, 0.16666666666666666, 0.3333333333333333, 0.17647058823529413, 0.25, 0.1111111111111111, 0.25, 0.25, 0.25, 0.17647058823529413, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.25, 0.17647058823529413, 0.25, 0.14285714285714285, 0.25, 0.42857142857142855, 0.5384615384615384, 0.25, 0.25, 0.5384615384615384, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.17647058823529413, 0.25, 0.3333333333333333, 0.17647058823529413, 0.25, 0.3333333333333333, 0.8181818181818182, 0.17647058823529413, 0.42857142857142855, 0.42857142857142855, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.1875, 0.25, 0.25, 0.3333333333333333, 0.17647058823529413, 0.1111111111111111, 0.25, 0.25, 0.3333333333333333, 0.25, 0.42857142857142855, 0.25, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.5, 0.5384615384615384, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.25, 0.3333333333333333, 0.3333333333333333, 0.46153846153846156, 0.3333333333333333, 0.17647058823529413, 0.05263157894736842, 0.1111111111111111, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.42857142857142855, 0.5384615384615384, 0.0, 0.25, 0.25, 0.25, 0.25, 0.25, 0.05555555555555555, 0.17647058823529413, 0.1111111111111111, 0.16666666666666666, 0.42857142857142855, 0.25, 0.3333333333333333, 0.25, 0.1111111111111111, 0.25, 0.3333333333333333, 0.5384615384615384, 0.10526315789473684, 0.3333333333333333, 0.42857142857142855, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.05263157894736842, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.05263157894736842, 0.1111111111111111, 0.25, 0.42857142857142855, 0.25, 0.25, 0.17647058823529413, 0.25, 0.1111111111111111, 0.3125, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.25, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.25, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111, 0.6666666666666666, 0.1111111111111111, 0.25, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.3333333333333333, 0.3333333333333333, 0.25, 0.6666666666666666, 0.25, 0.17647058823529413, 0.25, 0.3333333333333333, 0.42857142857142855, 0.05263157894736842, 0.5384615384615384, 0.29411764705882354, 0.25, 0.42857142857142855, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.3333333333333333, 0.25, 0.25, 0.11764705882352941, 0.1111111111111111, 0.17647058823529413, 0.375, 0.25, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.17647058823529413, 0.25, 0.42857142857142855, 0.05263157894736842, 0.1111111111111111, 0.25, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.25, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.5384615384615384, 0.17647058823529413, 0.25, 0.42857142857142855, 0.46153846153846156, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.1111111111111111, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.25, 0.17647058823529413, 0.25, 0.3333333333333333, 0.25, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.23076923076923078, 0.25, 0.1111111111111111, 0.1875, 0.1111111111111111, 0.42857142857142855, 0.3333333333333333, 0.1111111111111111, 0.25, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.35714285714285715, 0.3333333333333333, 0.25, 0.25, 0.5384615384615384, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.17647058823529413, 0.25, 0.11764705882352941, 0.25, 0.5384615384615384, 0.3333333333333333, 0.5384615384615384, 0.17647058823529413, 0.5384615384615384, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.25, 0.46153846153846156, 0.1111111111111111, 0.3333333333333333, 0.1875, 0.16666666666666666, 0.05, 0.3333333333333333, 0.17647058823529413, 0.16666666666666666, 0.17647058823529413, 0.26666666666666666, 0.17647058823529413, 0.1111111111111111, 0.25, 0.11764705882352941, 0.3333333333333333, 0.1111111111111111, 0.1111111111111111, 0.17647058823529413, 0.1111111111111111, 0.42857142857142855, 0.25, 0.3333333333333333, 0.25, 0.25, 0.17647058823529413, 0.25, 0.42857142857142855, 0.17647058823529413, 0.3333333333333333, 0.1111111111111111, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.3333333333333333, 0.17647058823529413, 0.42857142857142855, 0.42857142857142855, 0.05263157894736842, 0.23529411764705882, 0.5, 0.17647058823529413, 0.23529411764705882, 0.05263157894736842, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.25, 0.3333333333333333, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.05263157894736842, 0.17647058823529413, 0.25, 0.17647058823529413, 0.25, 0.42857142857142855, 0.25, 0.1875, 0.1111111111111111, 0.17647058823529413, 0.05263157894736842, 0.42857142857142855, 0.42857142857142855, 0.3333333333333333, 0.25, 0.11764705882352941, 0.42857142857142855, 0.05263157894736842, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.25, 0.17647058823529413, 0.25, 0.17647058823529413, 0.42857142857142855, 0.3333333333333333, 0.25, 0.3333333333333333, 0.42857142857142855, 0.25, 0.17647058823529413, 0.5384615384615384, 0.17647058823529413, 0.25, 0.29411764705882354, 0.25, 0.42857142857142855, 0.25, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.23529411764705882, 0.3333333333333333, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.25, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.17647058823529413, 0.25, 0.1111111111111111, 0.25, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.25, 0.17647058823529413, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.25, 0.5384615384615384, 0.17647058823529413, 0.42857142857142855, 0.42857142857142855, 0.25, 0.17647058823529413, 0.42857142857142855, 0.5384615384615384, 0.25, 0.25, 0.1111111111111111, 0.1111111111111111, 0.0, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.3333333333333333, 0.25, 0.1111111111111111, 0.25, 0.1111111111111111, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.05263157894736842, 0.25, 0.17647058823529413, 0.25, 0.25, 0.17647058823529413, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.6666666666666666, 0.25, 0.42857142857142855, 0.3333333333333333, 0.25, 0.3333333333333333, 0.5384615384615384, 0.25, 0.3333333333333333, 0.1111111111111111, 0.17647058823529413, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.17647058823529413, 0.42857142857142855, 0.6666666666666666, 0.25, 0.42857142857142855, 0.42857142857142855, 0.17647058823529413, 0.42857142857142855, 0.1111111111111111, 0.17647058823529413, 0.25, 0.3333333333333333, 0.05263157894736842, 0.17647058823529413, 0.1111111111111111, 0.6666666666666666, 0.42857142857142855, 0.25, 0.3333333333333333, 0.5384615384615384, 0.25, 0.42857142857142855, 0.25, 0.17647058823529413, 0.25, 0.25, 0.25, 0.17647058823529413, 0.3333333333333333, 0.25, 0.0, 0.05555555555555555, 0.42857142857142855, 0.25, 0.17647058823529413, 0.17647058823529413, 0.3333333333333333, 0.42857142857142855, 0.25, 0.42857142857142855, 0.3333333333333333, 0.05263157894736842, 0.3333333333333333, 0.4375, 0.3333333333333333, 0.25, 0.25, 0.25, 0.23529411764705882, 0.3333333333333333, 0.25, 0.25, 0.42857142857142855, 0.17647058823529413, 0.25, 0.05263157894736842, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.3333333333333333, 0.11764705882352941, 0.25, 0.17647058823529413, 0.42857142857142855, 0.25, 0.1111111111111111, 0.17647058823529413, 0.17647058823529413, 0.1111111111111111, 0.1111111111111111, 0.1875, 0.25, 0.1111111111111111, 0.25, 0.25, 0.10526315789473684, 0.3333333333333333, 0.05263157894736842, 0.16666666666666666, 0.3333333333333333, 0.25, 0.05263157894736842, 0.1111111111111111, 0.25, 0.0, 0.1111111111111111, 0.3333333333333333, 0.17647058823529413, 0.17647058823529413, 0.25, 0.17647058823529413]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2610242723256392"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity_for_batch(labels, predictions, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
