path:
  PROJECT_NAME: news-topic-keyphrase-generation-model-dev
  RUN_ID: v4_run_20
  TRAIN_DATA_PATH: data/model_dev/model_dev_v4_paust_t5_large_train.hf
  EVAL_DATA_PATH: data/model_dev/model_dev_v4_paust_t5_large_eval.hf
  MODEL_CHECKPOINT: paust/pko-t5-large # EleutherAI/polyglot-ko-1.3b // EleutherAI/polyglot-ko-5.8b /// paust/pko-t5-base // paust/pko-t5-large
  TOKENIZER_CHECKPOINT: False
  NOTEBOOK_NAME: ./train.py

global_args:
  batch_size: 3
  epochs: 100
  learning_rate: 3e-6 # at batch_size 8 # 3e-6, 12e-6, 24e-6, 48e-6, 96e-6, 192e-6 // 24e-6 for polyglot-ko // 3e-6 for paust/pko-t5-large
  early_stopping_patience: 25
  int8: False

training_args:
  report_to: wandb
  num_train_epochs: ${global_args.epochs}
  per_device_train_batch_size: ${global_args.batch_size}
  per_device_eval_batch_size: ${global_args.batch_size}
  gradient_accumulation_steps: 1
  optim: adamw_torch  # 'adamw_torch' or 'adamw_hf'
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.01
  gradient_checkpointing: False
  lr_scheduler_type: linear  # 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'
  warmup_ratio: 0
  save_total_limit: 2 # 1: last ckpt only, 2: best and last
  load_best_model_at_end: True
  metric_for_best_model: eval_loss
  save_strategy: epoch
  evaluation_strategy: epoch
  logging_strategy: steps
  logging_first_step: True
  # predict_with_generate: False
  # generation_max_length: 64
  # generation_num_beams: 5 # 1 is for greedy search
  fp16: False
  bf16: True
  tf32: False