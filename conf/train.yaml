path:
  PROJECT_NAME: news-topic-keyphrase-generation-model-dev
  RUN_ID: v4_run_3
  TRAIN_DATA_PATH: data/model_dev/model_dev_v4_polyglot_1.3b_train.hf
  EVAL_DATA_PATH: data/model_dev/model_dev_v4_polyglot_1.3b_eval.hf
  MODEL_CHECKPOINT: EleutherAI/polyglot-ko-1.3b # EleutherAI/polyglot-ko-1.3b // EleutherAI/polyglot-ko-5.8b /// paust/pko-t5-base
  NOTEBOOK_NAME: ./train.py

global_args:
  batch_size: 4
  epochs: 10
  learning_rate: 12e-6 # at batch_size 8 # 3e-6, 12e-6
  early_stopping_patience: 3
  int8: False

training_args:
  report_to: wandb
  num_train_epochs: ${global_args.epochs}
  per_device_train_batch_size: ${global_args.batch_size}
  per_device_eval_batch_size: ${global_args.batch_size}
  gradient_accumulation_steps: 1
  optim: adamw_torch  # 'adamw_torch' or 'adamw_hf'
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.01
  lr_scheduler_type: linear  # 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'
  warmup_ratio: 0
  save_total_limit: 2
  load_best_model_at_end: True
  metric_for_best_model: eval_loss
  save_strategy: epoch
  evaluation_strategy: epoch
  logging_strategy: steps
  logging_first_step: True
  # predict_with_generate: False
  # generation_max_length: 64
  # generation_num_beams: 5 # 1 is for greedy search
  fp16: False
  bf16: True
  tf32: False