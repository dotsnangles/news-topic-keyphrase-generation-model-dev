{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"  # Set the GPUs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, \n",
    "    AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGPU = torch.cuda.device_count()\n",
    "NCPU = os.cpu_count()\n",
    "NGPU, NCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths and names\n",
    "\n",
    "PROJECT_NAME = 'news-topic-keyphrase-generation-model-dev'\n",
    "RUN_ID = 'v3_run_7_adafactor_lora'\n",
    "\n",
    "TRAIN_DATA_PATH = 'data/model_dev/model_dev_v3_train.hf'\n",
    "EVAL_DATA_PATH = 'data/model_dev/model_dev_v3_eval.hf'\n",
    "\n",
    "MODEL_CHECKPOINT = 'paust/pko-t5-base'\n",
    "model_name = re.sub(r'[/-]', r'_', MODEL_CHECKPOINT).lower()\n",
    "\n",
    "NOTEBOOK_NAME = './train_seq2seq_plm.ipynb'\n",
    "\n",
    "ROOT_PATH = './'\n",
    "SAVE_PATH = os.path.join(ROOT_PATH, '.log')\n",
    "\n",
    "run_name = f'{model_name}_{RUN_ID}'\n",
    "output_dir = os.path.join(SAVE_PATH, run_name)\n",
    "\n",
    "print(run_name)\n",
    "print(output_dir)\n",
    "\n",
    "!mkdir -p {SAVE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_PROJECT'] = PROJECT_NAME\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = NOTEBOOK_NAME\n",
    "os.environ['WANDB_LOG_MODEL'] = 'false'\n",
    "os.environ['WANDB_WATCH'] = 'all'\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    report_to=\"wandb\",\n",
    "\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    ### AdaFactor\n",
    "    optim= 'adafactor',\n",
    "    learning_rate=3e-6 * (batch_size * NGPU) / 8,\n",
    "\n",
    "    lr_scheduler_type='linear', # 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'\n",
    "    warmup_ratio=0,\n",
    "\n",
    "    # ### AdamW\n",
    "    # optim= 'adamw_torch', # 'adamw_torch' or 'adamw_hf'\n",
    "    # learning_rate=3e-6, # 3e-6 * (per_device_train_batch_size * NGPU) / 8\n",
    "    # adam_beta1=0.9,\n",
    "    # adam_beta2=0.999,\n",
    "    # adam_epsilon=1e-8,\n",
    "    # weight_decay=0.01,\n",
    "\n",
    "    # lr_scheduler_type='linear', # 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'\n",
    "    # warmup_ratio=0,\n",
    "\n",
    "    save_total_limit=2,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True, \n",
    "    logging_steps=int(500 / NGPU),\n",
    "\n",
    "    predict_with_generate=False,\n",
    "    generation_max_length=64,\n",
    "    # generation_num_beams=generation_num_beams,\n",
    "\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(TRAIN_DATA_PATH)\n",
    "eval_dataset = load_from_disk(EVAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_dataset['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    \n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep = [\n",
    "#     'added_tokens.json',\n",
    "#     'config.json',\n",
    "#     'pytorch_model.bin',\n",
    "#     'special_tokens_map.json',\n",
    "#     'tokenizer.json',\n",
    "#     'tokenizer_config.json',\n",
    "#     'vocab.txt'\n",
    "# ]\n",
    "\n",
    "# ckpts = os.listdir(output_dir)\n",
    "# for ckpt in ckpts:\n",
    "#     ckpt = os.path.join(output_dir, ckpt)\n",
    "#     for item in os.listdir(ckpt):\n",
    "#         if item not in keep:\n",
    "#             os.remove(os.path.join(ckpt, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, pred in zip(eval_dataset, preds.predictions):\n",
    "#     pred = np.where(pred != -100, pred, tokenizer.pad_token_id)\n",
    "#     # context = tokenizer.decode(data['input_ids'], skip_special_tokens=True)\n",
    "#     summary = tokenizer.decode(data['labels'], skip_special_tokens=True)\n",
    "#     pred = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "#     # print(f'입력: {context}')\n",
    "#     print(f'정답: {summary}')\n",
    "#     print(f'예측: {pred}', end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
