{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, \n",
    "    AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import evaluate\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGPU = torch.cuda.device_count()\n",
    "NCPU = os.cpu_count()\n",
    "NGPU, NCPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths and names\n",
    "\n",
    "TRAIN_DATA_PATH = 'data/model_dev/model_dev_v3_train.hf'\n",
    "EVAL_DATA_PATH = 'data/model_dev/model_dev_v3_eval.hf'\n",
    "\n",
    "MODEL_CHECKPOINT = '.log/paust_pko_t5_base_v3_run_5/checkpoint-11310'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, config=config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9346\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2337\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_from_disk(TRAIN_DATA_PATH)\n",
    "eval_dataset = load_from_disk(EVAL_DATA_PATH)\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = eval_dataset[:100]\n",
    "inputs = eval_dataset\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer.batch_decode(inputs['labels'], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [01:58<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "predictions = None\n",
    "with torch.no_grad():\n",
    "    start = 0\n",
    "    for idx in tqdm(range(batch_size, len(input_ids), batch_size)):\n",
    "        ids, mask = input_ids[start:idx], attention_mask[start:idx]\n",
    "        ids, mask = torch.tensor(ids).to(device), torch.tensor(mask).to(device)\n",
    "        # print(start, idx)\n",
    "        prediction = model.generate(input_ids=ids, attention_mask=mask, max_length=64)\n",
    "        if predictions == None:\n",
    "            predictions = prediction.detach().cpu().tolist()\n",
    "        else:\n",
    "            predictions.extend(prediction.detach().cpu().tolist())\n",
    "        start = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_for_sampale(label, prediction):\n",
    "    return rouge.compute(references=[label], predictions=[prediction], tokenizer=komoran.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_for_batch(labels, predictions):\n",
    "    rouge_scores = None\n",
    "    \n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if rouge_scores == None:\n",
    "            rouge_scores = rouge_for_sampale(label, prediction)\n",
    "        else:\n",
    "            rouge_score = rouge_for_sampale(label, prediction)\n",
    "            for key in rouge_scores.keys():\n",
    "                rouge_scores[key] = rouge_scores[key] + rouge_score[key]\n",
    "    \n",
    "    for key in rouge_scores.keys():\n",
    "        rouge_scores[key] = rouge_scores[key] / len(labels)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6481759823542651,\n",
       " 'rouge2': 0.4481389571495031,\n",
       " 'rougeL': 0.5329436625976698,\n",
       " 'rougeLsum': 0.5329436625976698}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_for_batch(labels, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### V1\n",
    "\n",
    "# def f1_score_at_k_for_sample(label_str, prediction_str, k):\n",
    "#     true_positives = 0\n",
    "#     false_positives = 0\n",
    "#     false_negatives = 0\n",
    "    \n",
    "#     # convert label and prediction strings to sets of key-phrases\n",
    "#     label_lst = [key_phrase.strip() for key_phrase in label_str.split(';') if key_phrase != '']\n",
    "#     label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "#     label_set = set(label_lst)\n",
    "    \n",
    "#     # split the predicted key-phrases and their scores\n",
    "#     prediction_lst = [key_phrase.strip() for key_phrase in prediction_str.split(';') if key_phrase != '']\n",
    "#     prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "#     prediction_set = set(prediction_lst)\n",
    "    \n",
    "#     # calculate true positives, false positives, and false negatives\n",
    "#     for keyphrase in prediction_set:\n",
    "#         if keyphrase in label_set:\n",
    "#             true_positives += 1\n",
    "#         else:\n",
    "#             false_positives += 1\n",
    "    \n",
    "#     for keyphrase in label_set:\n",
    "#         if keyphrase not in prediction_set:\n",
    "#             false_negatives += 1\n",
    "    \n",
    "#     # calculate precision, recall, and F1 score\n",
    "#     precision = true_positives / (true_positives + false_positives)\n",
    "#     recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "#     if precision == 0 or recall == 0:\n",
    "#         return 0\n",
    "    \n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "#     return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### V2\n",
    "\n",
    "def f1_score_at_k_for_sample(label_str, prediction_str, k):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label_str.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction_str.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    \n",
    "    # calculate true positives, false positives, and false negatives\n",
    "    for keyphrase in prediction_lst:\n",
    "        similarity = False\n",
    "        for label in label_lst:\n",
    "            if keyphrase in label or label in keyphrase:\n",
    "                similarity = True\n",
    "                break\n",
    "        if similarity == True:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    for label in label_lst:\n",
    "        similarity = False\n",
    "        for keyphrase in prediction_lst:\n",
    "            if label in keyphrase or keyphrase in label:\n",
    "                similarity = True\n",
    "                break\n",
    "        if similarity == False:\n",
    "            false_negatives += 1            \n",
    "\n",
    "    # calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k_for_batch(labels, predictions, k):\n",
    "    f1_scores =[]\n",
    "\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        f1_scores.append(f1_score_at_k_for_sample(label, prediction, k))\n",
    "\n",
    "    # print(f1_scores)\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597830229966394"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_at_k_for_batch(labels, predictions, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score_at_k_for_sample(labels[9], prediction[9], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_for_sample(label, prediction, k):\n",
    "\n",
    "    # convert label and prediction strings to sets of key-phrases\n",
    "    label_lst = [key_phrase.strip() for key_phrase in label.split(';') if key_phrase != '']\n",
    "    label_lst = [key_phrase for key_phrase in label_lst if key_phrase != '']\n",
    "    # print(label_lst)\n",
    "    \n",
    "    # split the predicted key-phrases and their scores\n",
    "    prediction_lst = [key_phrase.strip() for key_phrase in prediction.split(';') if key_phrase != '']\n",
    "    prediction_lst = [key_phrase for key_phrase in prediction_lst if key_phrase != ''][:k]\n",
    "    # print(prediction_lst)\n",
    "\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(label_lst).intersection(prediction_lst)))\n",
    "    union = (len(label_lst) + len(prediction_lst)) - intersection\n",
    "\n",
    "    # print(union)\n",
    "    # print(intersection)\n",
    "\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_for_batch(labels, predictions, k):\n",
    "    jaccard_similarities =[]\n",
    "\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        jaccard_similarities.append(jaccard_similarity_for_sample(label, prediction, k))\n",
    "\n",
    "    # print(jaccard_similarities)\n",
    "    return sum(jaccard_similarities) / len(jaccard_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2610242723256392"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity_for_batch(labels, predictions, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
